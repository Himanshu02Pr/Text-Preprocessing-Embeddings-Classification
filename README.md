# Basic Text Preprocessing, Embeddings Creation & Classification

---

## Preprocessing and Embeddings
The text_preprocessing notebook showcases and explains basic preprocessing tasks like lowercasing, stopword removal, tokenization, lemmatization etc.
Embeddings_frequency_based notebook explains bag of words (BoW) and term frequency - inverse document frequency (TF-IDF) for embeddings.
Embeddings_prediction_based notebook explains Word2Vec technique for embeddings creation.

---

## Text Classification
Text classification notebook demonstrates sentiment analysis on IMDB dataset, determining the comments as positive or negative. It explains basic NLP techniques, comparing different models with different embedding types.

### Features

- Text preprocessing: tokenization, lowercasing, stopword removal, and lemmatization
- Feature extraction: Bag-of-Words, TF-IDF and Word2Vec embeddings
- Model training: Multinomial Naive Bayes and Random Forest
- Evaluation: Accuracy and Confusion matrix
- Optional support for n-grams to capture contextual information

---

# Results

With almost 87% accuracy, Multinomial Naive Bayes model with TF-IDF embeddings performed the best among all.
